<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep ladb</title>
    <link>https://cerisara.github.io/xtofsite/</link>
    <description>Recent content on Deep ladb</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>fr-FR</language>
    <copyright>All rights reserved - 2015</copyright>
    <lastBuildDate>Mon, 27 Jun 2016 18:02:45 +0200</lastBuildDate>
    <atom:link href="https://cerisara.github.io/xtofsite/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Comparison of Keras vs. TFlearn</title>
      <link>https://cerisara.github.io/xtofsite/post/tflearn/</link>
      <pubDate>Mon, 27 Jun 2016 18:02:45 +0200</pubDate>
      
      <guid>https://cerisara.github.io/xtofsite/post/tflearn/</guid>
      <description>&lt;div class=&#34;document&#34;&gt;


&lt;p&gt;TFLearn is a new abstract wrapper around Tensorflow.
I find it actually very similar to Keras, except that TFLearn only supports Tensorflow.
This is both good and bad, because it might be easier to bypass this abstraction layer and directly
code in Tensorflow; but well, you&#39;re stuck to TF.&lt;/p&gt;
&lt;p&gt;So I tested for one of my classification task a simple LSTM in both Keras and Tensorflow.
Here&#39;s the code for the model both in Keras and TFLearn:&lt;/p&gt;
&lt;pre class=&#34;literal-block&#34;&gt;
if False:
    # TFLearn code
    g=tflearn.input_data([None,maxlen,len(char2id)])
    g=tflearn.lstm(g,nhid)
    g=tflearn.fully_connected(g,len(da2id), activation=&#39;softmax&#39;)
    g=tflearn.regression(g,optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;)

    model = tflearn.DNN(g, clip_gradients=0., tensorboard_verbose=0)
    model.fit(Xtr, Ytr, validation_set=(Xte,Yte), n_epoch=10, show_metric=True, batch_size=128)
else:
    # Keras code
    model = Sequential()
    model.add(LSTM(nhid, input_shape=(maxlen, nchars), go_backwards=False, return_sequences=False))
    model.add(Dense(len(da2id)))
    model.add(Activation(&#39;softmax&#39;))
    model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;)
    model.fit(Xtr,Ytr,nb_epoch=10,validation_data=(Xte,Yte),show_accuracy=True)
&lt;/pre&gt;
&lt;p&gt;Of course, I&#39;ve used the Tensorflow backend for Keras.&lt;/p&gt;
&lt;p&gt;As you can see, both codes are very similar !
Furthermore, the input and output tensors are exactly the same in both cases, which is
nice because it&#39;s thus very easy to compare them.&lt;/p&gt;
&lt;p&gt;Both pieces of code run at approximately the same speed.
In terms of accuracy, as you can see on the following screenshots,
They give quite comparable results, with slightly better results for Keras.&lt;/p&gt;
&lt;p&gt;Output from TFlearn:&lt;/p&gt;
&lt;pre class=&#34;literal-block&#34;&gt;
Training samples: 104564
Validation samples: 2207
--
Training Step: 817  | total loss: 1.34866
| Adam | epoch: 001 | loss: 1.34866 - acc: 0.6172 | val_loss: 1.48790 - val_acc: 0.5221 -- iter: 000000/104564
--
Training Step: 1634  | total loss: 1.24601
| Adam | epoch: 002 | loss: 1.24601 - acc: 0.6445 | val_loss: 1.38776 - val_acc: 0.5452 -- iter: 000000/104564
--
Training Step: 2451  | total loss: 1.25703
| Adam | epoch: 003 | loss: 1.25703 - acc: 0.6359 | val_loss: 1.35823 - val_acc: 0.5452 -- iter: 000000/104564
--
Training Step: 3268  | total loss: 1.21172
| Adam | epoch: 004 | loss: 1.21172 - acc: 0.6435 | val_loss: 1.31693 - val_acc: 0.5470 -- iter: 000000/104564
--
Training Step: 4085  | total loss: 1.18737
| Adam | epoch: 005 | loss: 1.18737 - acc: 0.6375 | val_loss: 1.27222 - val_acc: 0.5478 -- iter: 000000/104564
--
Training Step: 4902  | total loss: 1.15483
| Adam | epoch: 006 | loss: 1.15483 - acc: 0.6420 | val_loss: 1.25492 - val_acc: 0.5501 -- iter: 000000/104564
--
Training Step: 5719  | total loss: 1.09600
| Adam | epoch: 007 | loss: 1.09600 - acc: 0.6510 | val_loss: 1.23394 - val_acc: 0.5538 -- iter: 000000/104564
--
Training Step: 6536  | total loss: 1.10121
| Adam | epoch: 008 | loss: 1.10121 - acc: 0.6576 | val_loss: 1.24449 - val_acc: 0.5608 -- iter: 000000/104564
--
Training Step: 7353  | total loss: 1.10770
| Adam | epoch: 009 | loss: 1.10770 - acc: 0.6427 | val_loss: 1.17806 - val_acc: 0.5846 -- iter: 000000/104564
--
Training Step: 8170  | total loss: 1.06616
| Adam | epoch: 010 | loss: 1.06616 - acc: 0.6594 | val_loss: 1.16912 - val_acc: 0.5816 -- iter: 000000/104564
&lt;/pre&gt;
&lt;p&gt;Output from Keras:&lt;/p&gt;
&lt;pre class=&#34;literal-block&#34;&gt;
Train on 104564 samples, validate on 2207 samples
Epoch 1/10
104564/104564 [==============================] - 38s - loss: 1.3849 - acc: 0.6192 - val_loss: 1.3353 - val_acc: 0.5478
Epoch 2/10
104564/104564 [==============================] - 38s - loss: 1.1533 - acc: 0.6468 - val_loss: 1.2243 - val_acc: 0.5741
Epoch 3/10
104564/104564 [==============================] - 38s - loss: 1.0937 - acc: 0.6581 - val_loss: 1.2146 - val_acc: 0.5696
Epoch 4/10
104564/104564 [==============================] - 38s - loss: 1.0583 - acc: 0.6637 - val_loss: 1.1613 - val_acc: 0.5863
Epoch 5/10
104564/104564 [==============================] - 38s - loss: 1.0369 - acc: 0.6685 - val_loss: 1.1848 - val_acc: 0.5872
Epoch 6/10
104564/104564 [==============================] - 38s - loss: 1.0219 - acc: 0.6717 - val_loss: 1.1340 - val_acc: 0.5922
Epoch 7/10
104564/104564 [==============================] - 38s - loss: 1.0066 - acc: 0.6752 - val_loss: 1.1303 - val_acc: 0.5949
Epoch 8/10
104564/104564 [==============================] - 38s - loss: 0.9965 - acc: 0.6774 - val_loss: 1.1235 - val_acc: 0.5967
Epoch 9/10
104564/104564 [==============================] - 38s - loss: 0.9867 - acc: 0.6788 - val_loss: 1.1135 - val_acc: 0.5995
Epoch 10/10
104564/104564 [==============================] - 38s - loss: 0.9781 - acc: 0.6809 - val_loss: 1.1198 - val_acc: 0.6044
&lt;/pre&gt;
&lt;p&gt;Disclaimer: this comparison is not reproducible, and is not meant to be, because
it relies on non-free data. But the code is so simple that you can reproduce this
experiment in less than 5 minutes with any dataset !
The results must be interpreted with caution, because they might be specific to these
particular experimental conditions, and may be very different in other conditions.
Note that no parameter has been tuned at all; results may also be very
different if parameters were tuned.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>About this site</title>
      <link>https://cerisara.github.io/xtofsite/post/intro/</link>
      <pubDate>Sun, 26 Jun 2016 18:02:45 +0200</pubDate>
      
      <guid>https://cerisara.github.io/xtofsite/post/intro/</guid>
      <description>&lt;p&gt;Nowadays, personal websites and blogs are a bit depreciated, because
everybody is posting on facebook. But a personal website is &lt;em&gt;yours&lt;/em&gt;, you
control all of it from start to end. And so, you can really be proud of it,
just like when you craft a small statue with your hands.
It&amp;rsquo;s just unique, it&amp;rsquo;s you.&lt;/p&gt;

&lt;p&gt;Over the years, I&amp;rsquo;ve tested many, many different solutions to build a website:
in particular pure html (but it&amp;rsquo;s really too much work, isn&amp;rsquo;t it ?)
and cms/wordpress (but you&amp;rsquo;re not in control, it&amp;rsquo;s heavy, no git, full of
vulnerabilities&amp;hellip;).
And I&amp;rsquo;ve come to a single conclusion: there is really nothing like static web
site generators: sites are fast, not vulnerable, modern-looking, git-friendly,
and you&amp;rsquo;re the boss. period.&lt;/p&gt;

&lt;p&gt;I started with the old &amp;ldquo;blogofile&amp;rdquo;, and switched to &lt;em&gt;Nikola&lt;/em&gt; after one year:
Nikola is really good, but after 2 years, I think it&amp;rsquo;s time to change, and
&lt;em&gt;Hugo&lt;/em&gt; looks best.&lt;/p&gt;

&lt;p&gt;A short recipe, if you&amp;rsquo;re a linux addict:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Install hugo&lt;/p&gt;

&lt;p&gt;sudo dkpg -i hugo.deb&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create your site directories:&lt;/p&gt;

&lt;p&gt;hugo new site mysite&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose and install a theme:&lt;/p&gt;

&lt;p&gt;cd mysite/themes
git clone &lt;a href=&#34;https://github.com/vjeantet/hugo-theme-casper&#34;&gt;https://github.com/vjeantet/hugo-theme-casper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add images in static/images/&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add a post with&lt;/p&gt;

&lt;p&gt;hugo new post/mypost.md
hugo undraft post/mypost.md&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compile and deploy&lt;/p&gt;

&lt;p&gt;hugo &amp;ndash;theme=hugo-theme-casper
scp/sft to you site host (github pages is your friend here)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enjoy !&lt;/p&gt;

&lt;p&gt;I also recommend you to version your site code with git.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Persistent live USB</title>
      <link>https://cerisara.github.io/xtofsite/post/liveusb/</link>
      <pubDate>Mon, 11 Jan 2016 18:02:45 +0200</pubDate>
      
      <guid>https://cerisara.github.io/xtofsite/post/liveusb/</guid>
      <description>

&lt;p&gt;How to simply create a bootable USB stick with a live Ubuntu 14 and persistency, UEFI-compatible,
and all of this without ever formatting or repartitionning the disk !&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t like repartitionning or reformating USB sticks, because they tend to be quite &amp;ldquo;fragile&amp;rdquo; these days:
when you buy them, they&amp;rsquo;re usually pre-partitionned with one big vfat partition.
So let&amp;rsquo;s keep with that, and install a bootable live Ubuntu with persistency on it.
Thus, you&amp;rsquo;ll still be able to use you USB stick on Windows, and in your favourite car CD-player device.&lt;/p&gt;

&lt;p&gt;In short, you&amp;rsquo;ll have to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;install GRUB on your USB stick&lt;/li&gt;
&lt;li&gt;copy the latest Ubuntu 14 iso on your USB stick&lt;/li&gt;
&lt;li&gt;create a casper-rw file in the root of your USB&lt;/li&gt;
&lt;li&gt;edit the grub configuration to boot from the iso and be persistent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that your USB may not boot with &amp;ldquo;legacy&amp;rdquo; boot mode, but at least it&amp;rsquo;s booting just fine with UEFI mode !
So you may have to edit the BIOS in order to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disable &amp;ldquo;secure boot&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Enable UEFI boot instead of legacy boot&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;create-the-persistent-file:eeef66a267cb8371553e45e319c50796&#34;&gt;Create the persistent file&lt;/h2&gt;

&lt;p&gt;Must be done on a different linux system than the one that is on the USB key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dd if=/dev/zero of=/path/to/casper-rw bs=1M count=512 

mkfs.ext3 /path/to/casper-rw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In your grub.txt on your USB key, edit your boot by adding the persisten keyword, to get something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;linux (loop)/casper/vmlinuz boot=casper iso-scan/filename=$isofile quiet splash noprompt persistent --
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>